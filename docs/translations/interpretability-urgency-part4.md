可解释性也非常适合学术和独立研究人员：它具有基础科学的风格，其中许多部分可以在不需要巨大计算资源的情况下研究。需要明确的是，一些独立研究人员和学者确实在研究可解释性，但我们需要更多。最后，如果你在其他科学领域并正在寻找新机会，可解释性可能是一个有前途的选择，因为它提供了丰富的数据、令人兴奋的新兴方法和巨大的现实世界价值。神经科学家尤其应该考虑这一点，因为收集人工神经网络的数据比生物神经网络容易得多，而且一些结论可以[应用回神经科学](https://pmc.ncbi.nlm.nih.gov/articles/PMC10055119/)。如果你有兴趣加入Anthropic的可解释性团队，我们有开放的[研究科学家](https://job-boards.greenhouse.io/anthropic/jobs/4020159008)和[研究工程师](https://job-boards.greenhouse.io/anthropic/jobs/4020305008)职位。

第二，政府可以**使用[轻触式规则](https://www.anthropic.com/news/the-case-for-targeted-regulation)来鼓励可解释性研究的发展**及其在解决前沿AI模型问题方面的应用。鉴于"AI MRI"的实践是多么新生和未发展，应该清楚为什么现阶段[监管或强制要求](https://www.documentcloud.org/documents/25003075-sia-sb-1047-anthropic/)公司进行这些检查是没有意义的：甚至不清楚一项前瞻性法律应该要求公司做什么。但要求公司透明披露其安全和安保实践（其负责任扩展政策，或RSP，及其执行），包括他们如何使用可解释性在发布前测试模型，将允许公司相互学习，同时也明确谁的行为更负责任，促进"向上竞争"。我们在[对加州前沿模型工作组草案报告的回应](https://www.anthropic.com/news/anthropic-s-response-to-governor-newsom-s-ai-working-group-draft-report)中建议将安全/安保/RSP透明度作为加州法律的可能方向（该报告本身也提到了一些相同的想法）。这个概念也可以在联邦层面或其他国家推广。

第三，政府可以使用**出口管制来创造一个"安全缓冲区"，可能给可解释性更多时间**在我们达到最强大的AI之前取得进展。我长期以来一直支持对[中国的芯片](https://www.wsj.com/opinion/trump-can-keep-americas-ai-advantage-china-chips-data-eccdce91)实施[出口管制](https://darioamodei.com/on-deepseek-and-export-controls)，因为我相信民主国家必须在AI方面保持领先于专制国家。但这些政策还有一个额外的好处。如果美国和其他民主国家在接近"数据中心里的天才国家"时有明显的AI领先优势，我们可能能够"花费"部分领先优势来确保可解释性在进入真正强大的AI之前处于更坚实的基础上，同时仍然击败我们的专制对手。即使是1-2年的领先优势，我相信有效且执行良好的出口管制可以给我们，可能意味着当我们达到变革性能力水平时，"AI MRI"基本上能工作与否的区别。一年前，我们无法追踪神经网络的思想，也无法识别其中数百万个概念；今天我们可以。相比之下，如果美国和中国同时达到强大的AI（这是我在没有出口管制的情况下预期会发生的），地缘政治激励将使任何放缓基本上不可能。

所有这些——加速可解释性、轻触式透明度立法和对中国芯片的出口管制——都有自身的优点，几乎没有明显的缺点。我们无论如何都应该做所有这些。但当我们意识到它们可能使可解释性在强大的AI之前或之后得到解决时，它们变得更加重要。

强大的AI将塑造人类的命运，我们应该在它们彻底改变我们的经济、生活和未来之前理解我们自己的创造物。

*感谢Tom McGrath、Martin Wattenberg、Chris Olah、Ben Buchanan以及Anthropic内部的许多人对本文草稿的反馈。*

## 脚注

1. 对于植物来说，这将是水、阳光、指向特定方向的支架、选择植物的种类等。这些因素大致决定了植物的生长位置，但其确切的形状和生长模式是不可预测的，即使在它们生长后也难以解释。对于AI系统，我们可以设置基本架构（通常是[Transformer](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)的某种变体）、它们接收的广泛数据类型以及用于训练它们的高级算法，但模型的实际认知机制是从这些成分中有机涌现的，我们对它们的理解很差。事实上，在自然和人工世界中，有许多我们在原则层面上理解（有时甚至控制）但在细节上不理解的系统：经济、雪花、元胞自动机、人类进化、人类大脑发育等等。

2. 当然，你可以尝试通过简单地与模型互动来检测这些风险，我们在实践中确实这样做。但因为欺骗恰恰是我们试图找到的行为，外部行为并不可靠。这有点像通过询问某人是否是恐怖分子来确定他们是否是恐怖分子——不一定没用，你可以从他们的回答方式和所说的话中学到东西，但显然不可靠。

3. 我可能会在未来的文章中更详细地描述这一点，但确实有很多实验（[其中许多](https://www.anthropic.com/research/reward-tampering)[是由Anthropic完成的](https://www.anthropic.com/research/alignment-faking)）表明，当模型的训练以某种人为方式引导时，它们可以在某些情况下撒谎或欺骗。也有一些看起来有点像"在考试中作弊"的真实世界行为的证据，尽管它比危险或有害更多的是退化。**没有**的是危险行为以更自然的方式涌现的证据，或者**普遍倾向**或**普遍意图**为了获得对世界的权力而撒谎和欺骗。正是后一点，看到模型内部可以帮助很多。

4. 至少在API服务模型的情况下。开放权重模型带来额外的危险，因为防护措施可以简单地被剥离。

5. 非常简短地说，你可能期望可解释性在两个方面与AI知觉和福利的关注交叉。首先，虽然心灵哲学是一个复杂且有争议的话题，但哲学家无疑会从AI模型中实际发生的详细描述中受益。如果我们认为它们是肤浅的模式匹配器，它们似乎不太可能值得道德考虑。如果我们发现它们执行的计算类似于动物甚至人类的大脑，这可能是支持道德考虑的证据。其次，也许最重要的是，如果我们曾经得出结论，AI模型的道德"患者身份"足够可信以保证行动，可解释性将发挥的作用。对AI的严肃道德核算不能信任它们的自我报告，因为我们可能会意外地训练它们假装没事，而实际上不是。在这种情况下，可解释性将在确定AI的福祉方面发挥关键作用。（事实上，从这个角度来看，已经有一些[轻微令人担忧的迹象](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html#safety-relevant-self)。）

6. 例如，以某种模糊的方式破解和理解人工神经网络内部发生的计算的想法可能在70多年前神经网络被发明时就已经存在，各种努力理解为什么神经网络以特定方式行为的尝试几乎存在了同样长的时间。但Chris不同寻常的地方在于**提出**并认真追求一项全面努力来理解它们做的**一切**。

7. 叠加的基本概念由[Arora等人](https://arxiv.org/pdf/1601.03764)在2016年描述，更一般地可以追溯到经典数学中的压缩感知工作。解释不可解释神经元的假设可以追溯到早期对视觉模型的机制可解释性工作。这时改变的是，很明显这将成为语言模型的一个中心问题，比视觉中更严重。我们能够提供一个[强有力的理论基础](https://transformer-circuits.pub/2022/toy_model/index.html)，确信叠加是正确的假设。

8. 一种说法是，可解释性应该像模型对齐的**测试集**，而传统的对齐技术如可扩展监督、RLHF、宪法AI等应该像**训练集**。也就是说，可解释性作为模型对齐的独立检查，不受训练过程的污染，训练过程可能会激励模型**看起来**对齐而实际上不是。这种观点的两个结果是：(a)我们应该非常谨慎地直接在生产中训练或优化可解释性输出（特征/概念、电路），因为这破坏了它们信号的独立性，(b)重要的是不要在一次生产运行中**过多次**使用诊断测试信号来通知训练过程的变化，因为这会逐渐将独立测试信号的信息泄露给训练过程（尽管比(a)慢得多）。换句话说，我们建议在评估官方、高风险的生产模型时，我们应该像对待隐藏评估或测试集那样谨慎对待可解释性分析。

9. 奇怪的是，机制可解释性有时似乎在学术界遇到实质性的文化阻力。例如，我担心有报道称一个非常受欢迎的机制可解释性ICML会议工作坊被以看似借口的理由[拒绝](https://x.com/neelnanda5/status/1902770659668955422)。如果属实，这种行为在AI学术界正在寻找保持相关性的方法的时候，是短视和自我挫败的。

10. 当然，还有其他减轻风险的技术——我不打算暗示可解释性是我们**唯一**的风险缓解工具。

11. 事实上，我相当怀疑即使在民主国家的公司之间，考虑到AI的惊人经济价值，任何减缓以解决风险的可能性。像这样正面对抗市场感觉就像试图用你的脚趾阻止一辆货运列车。但与倡导者的主张相反，我认为今天并不存在真正令人信服的危险证据，而且我实际上认为提供危险"确凿证据"的最可能途径是可解释性本身——这是投资它的另一个原因！
