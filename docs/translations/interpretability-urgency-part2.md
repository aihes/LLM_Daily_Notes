同样，关于AI模型滥用的担忧——例如，它们可能帮助恶意用户制造生物或网络武器，其方式超出了今天互联网上可以找到的信息——基于的想法是很难可靠地防止模型知道危险信息或泄露它们所知道的内容。我们可以对模型设置过滤器，但有大量可能的方式来"越狱"或欺骗模型，而发现越狱存在的唯一方法是通过经验找到它。如果能够查看模型内部，我们可能能够系统地阻止所有越狱，并且还能够描述模型拥有哪些危险知识。

AI系统的不透明性也意味着它们在许多应用中根本不被使用，例如高风险金融或安全关键设置，因为我们无法完全设定它们行为的限制，而少量错误可能非常有害。更好的可解释性可以大大提高我们设定可能错误范围的能力。事实上，对于某些应用来说，我们无法看到模型内部是采用它们的法律障碍——例如在抵押贷款评估中，法律要求决策必须是可解释的。同样，AI在科学领域取得了巨大进步，包括改进DNA和蛋白质序列数据的预测，但以这种方式预测的模式和结构通常难以被人类理解，也不能提供生物学见解。过去几个月的一些研究论文已经清楚地表明，可解释性[可以](https://arxiv.org/pdf/2412.12101)[帮助](https://www.biorxiv.org/content/10.1101/2025.02.18.638918v1)我们理解这些模式。

不透明性还有其他更奇特的后果，例如它阻碍了我们判断AI系统是否（或可能有一天）具有知觉并可能值得重要权利的能力。这是一个[足够复杂的话题](https://www.anthropic.com/research/exploring-model-welfare)，我不会详细讨论，但我怀疑它在未来会很重要。

## 机制可解释性的简史

由于上述所有原因，弄清楚模型在思考什么以及它们如何运作似乎是一项极其重要的任务。几十年来的传统观点认为这是不可能的，模型是不可理解的"黑盒"。我无法完全公正地讲述这种观点如何改变的完整故事，我的观点不可避免地受到我在谷歌、OpenAI和Anthropic亲眼所见的影响。但Chris Olah是最早尝试真正系统的研究计划来打开黑盒并理解其所有部件的人之一，这个领域现在被称为**机制可解释性**。Chris最初在谷歌，然后在OpenAI从事机制可解释性工作。当我们创立Anthropic时，我们决定将其作为新公司方向的核心部分，并且关键的是，将其集中在大语言模型上。随着时间的推移，这个领域已经发展壮大，现在包括几家主要AI公司的团队以及一些专注于可解释性的公司、非营利组织、学者和独立研究人员。简要总结一下该领域迄今为止的成就，以及如果我们想要应用机制可解释性来解决上述一些关键风险，还需要做什么，这会很有帮助。

机制可解释性的早期（2014-2020年）专注于视觉模型，能够识别模型内部的一些代表人类可理解概念的神经元，例如"汽车检测器"或"车轮检测器"，类似于早期神经科学假设和研究表明人脑有对应特定人物或概念的神经元，通常被普及为["詹妮弗·安妮斯顿"神经元](https://en.wikipedia.org/wiki/Grandmother_cell)（事实上，我们在AI模型中[发现了](https://distill.pub/2021/multimodal-neurons/)非常类似的神经元）。我们甚至能够发现这些神经元是如何连接的——例如，汽车检测器会寻找汽车下方的车轮检测器触发，并将其与其他视觉信号结合起来，以决定它正在看的物体是否确实是一辆汽车。

当Chris和我离开创立Anthropic时，我们决定将可解释性应用于新兴的语言领域，并在2021年开发了一些基本的[数学基础](https://transformer-circuits.pub/2021/framework/index.html)和[软件基础设施](https://transformer-circuits.pub/2021/garcon/index.html)来实现这一目标。我们立即在模型中发现了一些基本机制，这些机制执行解释语言所必需的工作：[复制和顺序模式匹配](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html)。我们还发现了一些[可解释的单个神经元](https://transformer-circuits.pub/2022/solu/index.html)，类似于我们在视觉模型中发现的那些，它们代表各种词汇和概念。然而，我们很快发现，虽然**一些**神经元可以立即解释，但绝大多数是许多不同词汇和概念的不连贯拼凑。我们将这种现象称为**叠加**，我们很快意识到模型可能包含数十亿个概念，但以一种我们无法理解的混乱方式。模型使用叠加是因为这允许它表达比神经元数量更多的概念，使其能够学习更多。如果叠加看起来纠缠不清且难以理解，那是因为，一如既往，AI模型的学习和操作完全没有针对人类可理解性进行优化。

解释叠加的困难阻碍了一段时间的进展，但最终我们[发现](https://transformer-circuits.pub/2023/monosemantic-features)（与[其他人](https://arxiv.org/pdf/2309.08600)同时）信号处理中一种现有的技术，称为**稀疏自编码器**，可以用来找到神经元的**组合**，这些组合确实对应于更清晰、更人类可理解的概念。这些神经元组合可以表达的概念远比单层神经网络的概念更加微妙：它们包括"字面或比喻地犹豫或迟疑"的概念，以及"表达不满的音乐流派"的概念。我们称这些概念为**特征**，并使用稀疏自编码器方法[映射](https://www.anthropic.com/research/mapping-mind-language-model)各种规模的模型中的特征，包括[现代最先进的模型](https://transformer-circuits.pub/2024/scaling-monosemanticity/)。例如，我们能够在中等规模的商业模型（Claude 3 Sonnet）中找到超过3000万个特征。此外，我们采用了一种称为[**自解释性**](https://openai.com/index/language-models-can-explain-neurons-in-language-models/)的方法——使用AI系统本身来分析可解释性特征——来扩展不仅找到特征，而且列出并识别它们在人类术语中的含义的过程。
