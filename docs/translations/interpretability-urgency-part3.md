找到并识别3000万个特征是一个重大进步，但我们相信即使在一个小型模型中实际上可能有**十亿**或更多的概念，所以我们只发现了可能存在的一小部分，这方面的工作仍在进行中。更大的模型，如Anthropic最强大产品中使用的那些，更加复杂。

一旦找到一个特征，我们可以做的不仅仅是观察它的作用——我们可以增加或减少它在神经网络处理中的重要性。可解释性的MRI可以帮助我们开发和完善干预措施——几乎就像刺激某人大脑的特定部分。最令人难忘的是，我们使用这种方法创建了["金门大桥Claude"](https://www.anthropic.com/news/golden-gate-claude)，这是Anthropic一个模型的版本，其中"金门大桥"特征被人为放大，导致模型对这座桥梁着迷，甚至在不相关的对话中也会提到它。

最近，我们已经从跟踪和操作特征转向跟踪和操作[我们称为"电路"的特征组](https://transformer-circuits.pub/2025/attribution-graphs/biology.html)。这些电路显示了模型思考的步骤：概念如何从输入词汇中涌现，这些概念如何相互作用形成新概念，以及它们如何在模型中工作以生成行动。通过电路，我们可以"追踪"模型的思考。例如，如果你问模型"包含达拉斯的州的首府是什么？"，有一个"位于内部"的电路会导致"达拉斯"特征触发"德克萨斯"特征的激活，然后有一个电路会导致"奥斯汀"在"德克萨斯"和"首府"之后激活。尽管我们只通过手动过程找到了少量电路，但我们已经可以用它们来看到模型如何推理问题——例如它如何在写诗时提前计划押韵，以及它如何在不同语言之间共享概念。我们正在研究自动化寻找电路的方法，因为我们预计模型内部有数百万个电路以复杂的方式相互作用。

## 可解释性的效用

所有这些进展，虽然在科学上令人印象深刻，但并没有直接回答如何使用可解释性来减少我之前列出的风险的问题。假设我们已经确定了一堆概念和电路——假设，甚至，我们知道所有这些，并且我们能够比今天更好地理解和组织它们。那又怎样？我们如何**使用**所有这些？从抽象理论到实际价值仍然存在差距。

为了帮助缩小这个差距，我们开始尝试使用我们的可解释性方法来发现和诊断模型中的问题。最近，我们进行了[一项实验](https://www.anthropic.com/research/auditing-hidden-objectives)，其中"红队"故意在模型中引入对齐问题（比如，模型倾向于利用任务中的漏洞），并给各种"蓝队"任务，让他们找出问题所在。多个蓝队成功了；特别相关的是，其中一些在调查过程中有效地应用了可解释性工具。我们仍然需要扩展这些方法，但这个练习帮助我们获得了一些使用可解释性技术来发现和解决模型缺陷的实际经验。

我们的长期愿望是能够查看最先进的模型，并基本上进行"大脑扫描"：一种检查，有很高的概率识别出广泛的问题，包括撒谎或欺骗的倾向、寻求权力、越狱漏洞、模型整体的认知优势和劣势等等。然后，这将与各种训练和对齐模型的技术一起使用，有点像医生可能做MRI来诊断疾病，然后开药治疗，然后再做另一次MRI来查看治疗进展，等等。很可能，我们测试和部署最强大模型（例如，那些在我们的[负责任扩展政策](https://www.anthropic.com/news/announcing-our-updated-responsible-scaling-policy)框架中处于AI安全级别4的模型）的关键部分是执行和正式化此类测试。

## 我们能做什么

一方面，最近的进展——特别是关于电路和基于可解释性的模型测试的结果——让我感觉我们即将在可解释性方面取得重大突破。尽管我们面前的任务是艰巨的，但我可以看到一条现实的道路，使可解释性成为一种复杂可靠的方式来诊断甚至非常先进的AI中的问题——一种真正的"AI的MRI"。事实上，按照目前的轨迹，我会强烈押注可解释性在5-10年内达到这一点。

另一方面，我担心AI本身发展如此之快，我们甚至可能没有这么多时间。正如我在[其他地方](https://darioamodei.com/machines-of-loving-grace)所写的，我们可能早在2026年或2027年就会有相当于"数据中心里的天才国家"的AI系统。我非常担心在没有更好地掌握可解释性的情况下部署这样的系统。这些系统将绝对是经济、技术和国家安全的核心，并且将能够如此多的自主性，以至于**我认为人类完全不知道它们如何工作基本上是不可接受的。**

因此，我们正处于可解释性和模型智能之间的竞赛中。这不是一个非此即彼的问题：正如我们所见，可解释性的每一次进步都定量地增加了我们查看模型内部和诊断其问题的能力。我们拥有的这类进步越多，"数据中心里的天才国家"发展良好的可能性就越大。AI公司、研究人员、政府和社会可以做几件事来改变局势：

首先，公司、学术界或非营利组织的AI研究人员可以**通过直接研究可解释性来加速它**。可解释性得到的关注比模型发布的持续洪流少，但它可能更重要。对我来说，现在也感觉是加入这个领域的理想时机：最近的["电路"结果](https://transformer-circuits.pub/2025/attribution-graphs/biology.html)已经开辟了许多并行方向。Anthropic正在加倍投入可解释性，我们的目标是到2027年实现"可解释性能可靠地检测大多数模型问题"。我们也在投资[可解释性初创公司](https://www.theinformation.com/articles/anthropic-invests-startup-decodes-ai-models?rc=x8tsuw)。

但成功的机会更大，如果这是一项跨越整个科学界的努力。其他公司，如[Google DeepMind](https://ai.google.dev/gemma/docs/gemma_scope)和[OpenAI](https://arxiv.org/abs/2406.04093)，也有一些可解释性努力，但我强烈鼓励他们分配更多资源。如果有帮助的话，Anthropic将尝试将可解释性商业化应用，创造独特优势，特别是在需要为决策提供解释的行业。如果你是竞争对手，不希望这种情况发生，你也应该更多地投资可解释性！
